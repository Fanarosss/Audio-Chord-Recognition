{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Audio Chord Estimation using Neural Networks</h1>\n",
    "<h4>Theofanis Aslanidis</h4>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Abstract</h3>\n",
    "In this project, we are going to explore the use of deep neural networks for recognizing audio chords, using the Isophonics dataset - the one that is used on MIREX.<br>\n",
    "For the first steps of the implementation, I'm using a test-dataset only with one album, because the memory needed is very large, due to the one hot representation. So I'm using only the album \"<i>Let it Be</i>\" until I implement a batch training, where I won't need to load all data on memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# do this only once\n",
    "sys.path.append('./Project/audio-processing/')\n",
    "sys.path.append('./Project/data-processing/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#python input/output and regex\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "#signal processing libraries\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "#sklearn for normalization\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#librosa\n",
    "import librosa\n",
    "\n",
    "#import custom modules\n",
    "import filters\n",
    "import spectrograms\n",
    "import audiofiles\n",
    "from annotation_processing import chords_to_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Audio File pre-processing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to Convert <b>mp3 files to wav</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofiles.mp3_to_wav()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to Convert files from <b>stereo to mono</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofiles.stereo_to_mono()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading Datasets</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the <b>isophonics</b> dataset, in the chordlab folder, there are all the annotations we need.<br>\n",
    "Read all those chordlab files as <b>pandas dataframes</b> and store them in a dictionary of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all chordlab files\n",
    "Chordlab = {'The Beatles': {}}\n",
    "for filename in Path('Test-Dataset/The Beatles').glob('**/chordlab/**/*.lab'):\n",
    "    \n",
    "    path, track = os.path.split(filename)\n",
    "    path, album = os.path.split(path)\n",
    "    track_no = re.search('([0-9].)_-_',track).group(1)\n",
    "    \n",
    "    if (album not in Chordlab['The Beatles']): \n",
    "        Chordlab['The Beatles'][album] = {}\n",
    "        \n",
    "    Chordlab['The Beatles'][album][track_no] = pd.read_csv(filename, names=['Starts', 'Ends', 'Chord'], sep=' ', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the tracks we have the annotations, we need to load the appropriate audio track for each one, create the spectrogram, and store the (time, frequency, power) vector.<br>\n",
    "<b>Steps:</b>\n",
    "<ul>\n",
    "    <li> Browse to those wav files\n",
    "    <li> Read with wavfile from SciPy\n",
    "    <li> Create spectrogram with plt.specgram\n",
    "</ul>\n",
    "<br>\n",
    "<b>Note:</b> For the spectrogram there are 2 ways to go,<br>\n",
    "<i>1) plt.specgram</i> (+) seems fast (-) less detailed<br>\n",
    "<i>2) Scipy signal.spectrogram</i> (+) more detailed (-) slow<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### get all audio files and create spectrogram for each track\n",
    "n_fft = 1024\n",
    "nperseg = 1024\n",
    "Spectrograms, frequencies_num = spectrograms.create_spectrograms(n_fft, nperseg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting spectrograms contain info for 513 frequency range with an overlap of 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data preprocessing, convert data into input that I can feed in the neural network</h3>\n",
    "<br>\n",
    "<ol>\n",
    "    <li> For training we should have all anotations, together with their spectrograms'\n",
    "    <li> For testing all the spectrograms of the test dataset\n",
    "</ol>\n",
    "<br>\n",
    "<b>Shape of the final training data</b>\n",
    "<ul>\n",
    "    <li>X_train, Y_train = [spectrograms], [annotations]\n",
    "    <li>X_test = [spectrograms_test]\n",
    "</ul>\n",
    "<br>\n",
    "<b>Steps to convert data from dictionaries into data that I can feed in the neural network</b>\n",
    "<br>\n",
    "Firstly, I'm thinking I have to create a vocabulary for the chords.<br>\n",
    "Such as the <b>word2vec</b> representation we use on NLP, we will need in this project something similar.<br>\n",
    "<b>Chord:</b><br>\n",
    "C -> 0<br>\n",
    "C#\\Db -> 1<br>\n",
    "D -> 2<br>\n",
    "D#\\Eb -> 3<br>\n",
    "E -> 4<br>\n",
    "F -> 5<br>\n",
    "F#\\Gb -> 6<br>\n",
    "etc.<br>\n",
    "<br>\n",
    "<b>Mode:</b>\n",
    "Minor Chord: 0<br>\n",
    "Major Chord: 1<br>\n",
    "<br>\n",
    "<b>Sustained: </b>\n",
    "1,2,3,4,5,6,7,8,9<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it seems, the combinations can vary.<br>\n",
    "So we can:\n",
    "<ul>\n",
    "    <li>try an encoding(hashing) for each chord, and when we stumble upon each chord we hash it to a unique combination that goes into a vector.</li>\n",
    "    <li>we can iterate all train data, and for each chord we see, create a \"slot\" in our dictionary. So we have a dictionary with all the chords that appear on the dataset. That way we can work with a <b>one hot</b> representation.</li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Chords in our Dataset:  247\n"
     ]
    }
   ],
   "source": [
    "### Load all chords in a dictionary\n",
    "Chords = []\n",
    "for album in Chordlab['The Beatles'].keys():\n",
    "    for track_no in Chordlab['The Beatles'][album].keys():\n",
    "        for index, row in Chordlab['The Beatles'][album][track_no].iterrows():\n",
    "            if row['Chord'] not in Chords: Chords.append(row['Chord'])\n",
    "                \n",
    "### How many chords do we have in our dataset?\n",
    "print (\"Unique Chords in our Dataset: \",len(Chords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one album we have 68 Chords, so our vectors are going to be <b>one hot vectors</b> of 68 size. (67 zeros and 1 one)<br>\n",
    "Create one hot encodings of the chords, with <b>pandas Series + get_dummies</b><br>\n",
    "<h4>One Hot Encodings</h4>\n",
    "<br>\n",
    "With <b>pandas</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chords = len(Chords)\n",
    "s = pd.Series(Chords)\n",
    "OneHotEncodings = pd.get_dummies(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With <b>Sklearn</b>:<br>\n",
    "<i>(which provides us with inverse transformation, for the results.)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "              n_values=None, sparse=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoder.fit(np.array(pd.Series(Chords)).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Indexing chords with timestamps</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Vector_flow.png\" width=\"800\" />\n",
    "\n",
    "<caption><center> <b>Figure 1</b>: Annotation Data Processing.</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to align the <b>timing of the chords</b> with the <b>times array</b><br>\n",
    "To achieve that lets try to convert the timing - chords data of the .lab files in the same representation as the spectrograms data.<br>\n",
    "We will keep the same timeline (times), and using the timing<br>\n",
    "<b><font color='red'>Warning</font>: This is computational expensive for the cpu. If you have it ready, you can load it below with pickle.</b><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chords2vec = chords_to_onehot(encoder, Spectrograms, Chordlab, Chords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our 4D data ready in a dictionary to go into our model for training.<br>\n",
    "3 dimensions (times, frequencies, power) will go as X_train and chords2vec will go as Y_train<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Store - Load Data with pickle</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STORE\n",
    "import pickle\n",
    "\n",
    "with open('let_it_Be.pickle', 'wb') as handle:\n",
    "    pickle.dump(chords2vec, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD\n",
    "import pickle\n",
    "\n",
    "with open('let_it_Be.pickle', 'rb') as handle:\n",
    "    chords2vec = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article the following is stated.<br>\n",
    "https://tm3.ghost.io/2018/09/05/building-lstms-for-time-series-forecasting/<br>\n",
    "<i>\"Instead of one sequence of 1870, you could have many sequences of let's say 20. Your sequences should be overlapping windows `[0-20], [1-21], [2-22]`, etc, so your final shape would be something like `(1850, 20, 14)`.Same process for your test data. Break into subsequences of the same length as training. You will have to play around with finding what a good subsequence length is. It is extremely important to have many different ways of slicing your data. If you train on just one super long sequence it will probably not learn anything interesting.\"</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train/Test Data Initialization</h3>\n",
    "<br>\n",
    "After appending all the data together, we slice them into batches, as shown below:<br>\n",
    "(Example is for the X_train, we do exactly the same for the Y_train - chord annotations.)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/slicing.png\" width=\"800\" />\n",
    "\n",
    "<caption><center> <b>Figure 2</b>: Slicing data into Batches.</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1. Constructing numpy arrays</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(535663, 513) (535663, 247)\n"
     ]
    }
   ],
   "source": [
    "x_initial_train = np.zeros((1,frequencies_num))\n",
    "y_initial_train = np.zeros((1,len(Chords)))\n",
    "x_initial_test = np.zeros((1,frequencies_num))\n",
    "y_initial_test = np.zeros((1,len(Chords)))\n",
    "test_track_no = '07'\n",
    "for album in chords2vec.keys():\n",
    "    for track_no in chords2vec[album].keys():\n",
    "        if track_no != '07' or album != '12_-_Let_It_Be':\n",
    "            y_initial_train = np.append(y_initial_train, chords2vec[album][track_no].T, axis = 0)\n",
    "            x_initial_train = np.append(x_initial_train, Spectrograms['The Beatles'][album][track_no]['powerSpectrum'].T, axis = 0)\n",
    "        else:\n",
    "            y_initial_test = np.append(y_initial_test, chords2vec[album][track_no].T, axis = 0)\n",
    "            x_initial_test = np.append(x_initial_test, Spectrograms['The Beatles'][album][track_no]['powerSpectrum'].T, axis = 0)\n",
    "\n",
    "print (x_initial_train.shape, y_initial_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>If you have limited RAM clear this 4GB dictionary for memory issues !</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spectrogram dict is no longer necessary\n",
    "Spectrograms.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2. Normalization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# fit scaler on training dataset\n",
    "scaler.fit(x_initial_train)\n",
    "# transform training dataset\n",
    "x_initial_train = scaler.transform(x_initial_train)\n",
    "\n",
    "# fit scaler on training dataset\n",
    "scaler.fit(x_initial_test)\n",
    "# transform training dataset\n",
    "x_initial_test = scaler.transform(x_initial_test)\n",
    "\n",
    "# keras normalization\n",
    "# x_initial_train = keras.utils.normalize(x_initial_train, axis = 0, order = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3. Slicing in batches of timeseries and for the LSTM</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the chunk size\n",
    "chunk_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1073, 500, 513)\n",
      "(1073, 500, 247)\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "mod = x_initial_train.shape[1] // chunk_size + 1 #chunk_size is the number of timestemps in each batch\n",
    "x_train = np.zeros((1,chunk_size,frequencies_num)) #num of frequencies\n",
    "y_train = np.zeros((1,chunk_size,len(Chords)))\n",
    "\n",
    "timestep = 0\n",
    "while timestep < x_initial_train.shape[0] :\n",
    "    batch_x = np.resize(x_initial_train[timestep:timestep+chunk_size,:], (1, chunk_size, frequencies_num)) #num of frequencies\n",
    "    batch_y = np.resize(y_initial_train[timestep:timestep+chunk_size,:], (1, chunk_size,len(Chords)))\n",
    "    \n",
    "    x_train = np.append(x_train, batch_x, axis = 0)\n",
    "    y_train = np.append(y_train, batch_y, axis = 0)\n",
    "    timestep += chunk_size\n",
    "    \n",
    "print (x_train.shape)\n",
    "print (y_train.shape)\n",
    "\n",
    "#delete first line batch of array because its zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 500, 513)\n",
      "(5, 500, 247)\n"
     ]
    }
   ],
   "source": [
    "x_test = np.zeros((1,chunk_size,frequencies_num)) #num of frequencies\n",
    "y_test = np.zeros((1,chunk_size,len(Chords)))\n",
    "\n",
    "timestep = 0\n",
    "while timestep < x_initial_test.shape[0] :\n",
    "    batch_x = np.resize(x_initial_test[timestep:timestep+chunk_size,:], (1, chunk_size, frequencies_num)) #num of frequencies\n",
    "    batch_y = np.resize(y_initial_test[timestep:timestep+chunk_size,:], (1, chunk_size,len(Chords)))\n",
    "    \n",
    "    x_test = np.append(x_test, batch_x, axis = 0)\n",
    "    y_test = np.append(y_test, batch_y, axis = 0)\n",
    "    timestep += chunk_size\n",
    "    \n",
    "print (x_test.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "#delete first line batch of array because its zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the first row from every array because of the append, which left it all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1072, 500, 513)\n",
      "(1072, 500, 247)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.delete(x_train,0,0)\n",
    "y_train = np.delete(y_train,0,0)\n",
    "x_test = np.delete(x_test,0,0)\n",
    "y_test = np.delete(y_test,0,0)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TensorFlow model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define first an RNN with tensorflow<br>\n",
    "- Layers = dimensionality of the output space\n",
    "- input shape = for every time step the feature shape\n",
    "- Dense output shape is the chord num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(chords_num, frequencies_num, timesteps, batch_size):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.LSTM(128, batch_input_shape = (batch_size, timesteps, frequencies_num), dropout=0.2, recurrent_dropout=0.3, return_sequences=True, stateful=True))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.3, return_sequences=True, stateful=True)))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(chords_num, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 4\n",
    "\n",
    "model = RNN(len(Chords), frequencies_num, chunk_size, batch_size)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train\n",
    "model.fit(x_train, y_train, epochs=epochs,\n",
    "          validation_data=(x_test, y_test), batch_size=batch_size, verbose=1)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test loss and accuracy : [2.437735080718994, 0.258]\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Final test loss and accuracy :\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transform predictions into Chords</h3>\n",
    "<br>\n",
    "Get the resulting one hot encodings, map to the appropriate chords and write the result<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimated Chords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1996</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1997</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1998</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1999</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Estimated Chords\n",
       "0                   C\n",
       "1                   G\n",
       "2                   G\n",
       "3                   G\n",
       "4                   G\n",
       "...               ...\n",
       "1995                G\n",
       "1996                G\n",
       "1997                G\n",
       "1998                D\n",
       "1999                D\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_chord_list = []\n",
    "real_chord_list = Chordlab['The Beatles']['12_-_Let_It_Be'][test_track_no]\n",
    "\n",
    "for batch_chords in predictions:\n",
    "    for chord in batch_chords:\n",
    "        estimated_chord_list.append(encoder.inverse_transform([chord]).reshape(1,)[0])\n",
    "        \n",
    "df_predictions = pd.DataFrame({'Estimated Chords' : estimated_chord_list})\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spectrograms</h2>\n",
    "<br>\n",
    "<b>We are going to use fewer timesteps in order to save memory.</b>\n",
    "<br>We will try and take the exact timestep when a chord changes and its neighborhood we can see some results.<br><br>\n",
    "<b>OBSERVATION!</b> Maybe classifying every time step that we did is wrong, and we should find out when a chord changes and from the various classifications of its neighborhood, we decide the final Chord for each interval.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
